{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792d37f2-5ced-487a-a62b-d1fc018e6bac",
   "metadata": {},
   "source": [
    "# Who has a voice in the media?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f5913-1bd6-47fb-b803-5889a0d383c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-processing the dataset\n",
    "To start with, we remove the rows of the dataset where either the author or the quotation is NaN. In addition, we remove the authors who probability is lower than 50%. As our whole analysis of \"who has a voice in the media\" is all about the speaker and what it has said, it makes no sense to take these rows into account.\n",
    "\n",
    "Later, we also do a sanity controll and remove possible duplicate of rows with the same quote-ID as we obiously don't want to use exactly the same quote more than once in our analyzes. \n",
    "\n",
    "Finally, to reduce the dataset further we remove columns that we will not use for our analysis: _quoteID_, _speaker_, _probas_, _urls_, _phase_ and _numOccurrences_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99d0b3f7-8038-40c9-974f-4bfef3a85fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started to process chunks...\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348775 rows with NaN speaker or quotation;\n",
      "- Dropped 32474 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.005 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348985 rows with NaN speaker or quotation;\n",
      "- Dropped 32299 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.004 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348778 rows with NaN speaker or quotation;\n",
      "- Dropped 32428 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.002 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348514 rows with NaN speaker or quotation;\n",
      "- Dropped 32595 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.004 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 347557 rows with NaN speaker or quotation;\n",
      "- Dropped 32485 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.000 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349038 rows with NaN speaker or quotation;\n",
      "- Dropped 32242 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.004 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349151 rows with NaN speaker or quotation;\n",
      "- Dropped 32310 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.002 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348451 rows with NaN speaker or quotation;\n",
      "- Dropped 32386 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.013 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348468 rows with NaN speaker or quotation;\n",
      "- Dropped 32421 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.001 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 347946 rows with NaN speaker or quotation;\n",
      "- Dropped 32403 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.012 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349276 rows with NaN speaker or quotation;\n",
      "- Dropped 32570 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.005 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348480 rows with NaN speaker or quotation;\n",
      "- Dropped 32543 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.000 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349176 rows with NaN speaker or quotation;\n",
      "- Dropped 32489 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.002 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348656 rows with NaN speaker or quotation;\n",
      "- Dropped 32127 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.000 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 347937 rows with NaN speaker or quotation;\n",
      "- Dropped 32208 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.003 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348633 rows with NaN speaker or quotation;\n",
      "- Dropped 32322 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.010 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349130 rows with NaN speaker or quotation;\n",
      "- Dropped 32487 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.011 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349290 rows with NaN speaker or quotation;\n",
      "- Dropped 32032 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.015 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348973 rows with NaN speaker or quotation;\n",
      "- Dropped 32274 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.005 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348629 rows with NaN speaker or quotation;\n",
      "- Dropped 32247 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.001 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349223 rows with NaN speaker or quotation;\n",
      "- Dropped 32620 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.007 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348675 rows with NaN speaker or quotation;\n",
      "- Dropped 32329 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.016 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348090 rows with NaN speaker or quotation;\n",
      "- Dropped 32133 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.016 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348868 rows with NaN speaker or quotation;\n",
      "- Dropped 31914 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.010 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348348 rows with NaN speaker or quotation;\n",
      "- Dropped 32276 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.004 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348372 rows with NaN speaker or quotation;\n",
      "- Dropped 32343 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.003 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 212871 rows with NaN speaker or quotation;\n",
      "- Dropped 19734 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.003 seconds.\n",
      "\n",
      "DONE processing all chunks and saving as csv after -0.000 minutes.\n",
      "THE END!\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def clean_data(chunk, thresh=0.5):\n",
    "    \n",
    "    # Drop duplicate quoteIDs\n",
    "    nr_rows = chunk.shape[0]\n",
    "    chunk = chunk.drop_duplicates(subset=['quoteID'])\n",
    "    print('- Dropped {} duplicate rows with same quoteID;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Drop quotes where either speaker or quotation is None\n",
    "    nr_rows = chunk.shape[0]\n",
    "    chunk.replace(to_replace=['None'], value=np.nan, inplace=True)\n",
    "    chunk = chunk.dropna(axis=0, subset=['speaker', 'quotation'])\n",
    "    print('- Dropped {} rows with NaN speaker or quotation;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Drop rows where speakers has probability less than 50%\n",
    "    nr_rows = chunk.shape[0]\n",
    "    prob_filter = pd.Series([(float(chunk.iloc[i].probas[0][1]) > thresh) for i in range(nr_rows)])\n",
    "    prob_filter = pd.Series(prob_filter)\n",
    "    chunk = chunk[prob_filter.values]\n",
    "    print('- Dropped {} rows with speaker prob smaller than 50%;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Remove columns we don't care about\n",
    "    chunk = chunk.drop(columns=['speaker', 'probas'])\n",
    "\n",
    "    return chunk\n",
    "\n",
    "start_of_all = timer()\n",
    "read_from_file = 'data/quotes-2015.json-002.bz2'\n",
    "write_to_file = 'data/clean-quotes-2015-updated.bz2'\n",
    "with pd.read_json(read_from_file, lines=True, compression='bz2', chunksize=1_000_000) as df_reader:\n",
    "    print('Started to process chunks...')\n",
    "    i = 0\n",
    "    for chunk in df_reader:\n",
    "        print('\\nProcessing new chunk...')\n",
    "        start = timer()\n",
    "        processed_chunk = clean_data(chunk)\n",
    "        processed_chunk.to_csv(write_to_file, compression='bz2', mode='a', index=False)\n",
    "        end = timer()\n",
    "        print('Done processing and saving chunk after {:.3f} seconds.'.format(end - start))\n",
    "        \n",
    "end_of_all = timer()\n",
    "print('\\nDONE processing all chunks and saving as csv after {:.3f} minutes.'.format((end_of_all - start_of_all) / 60))\n",
    "print('THE END!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43458793-75fc-44c6-9acb-263886e3a5d4",
   "metadata": {},
   "source": [
    "#### Short discussion around pre-processing\n",
    "Around one third of the original dataset has either a NaN quotation field, a NaN speaker, or a speaker with lower than 50% probability of having said that quote. Another one third of the original data is removed by the removal of the unwanted columns. Thus we are left with one third of the original dataset and still with full possibility of doing the wanted analysis\n",
    "\n",
    "Elsemore, it seems like there are no duplicates of quote-IDs in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186db5b0-6b8f-4ebe-9778-30efd28eba32",
   "metadata": {},
   "source": [
    "### Creating the Speakers dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c06ca-02e8-461d-80e0-ee7d603c62ba",
   "metadata": {},
   "source": [
    "In order to analyse who has a voice in the media we add a new column \"n_quotes\" to the wikidata-dataset which is how many times that person is present in the quotebank dataset from 2015 to 2020. This new dataset is saved in \"speakers.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd54c024-8614-4689-8a21-b843008d1e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "datafolder = Path(\"data\")\n",
    "\n",
    "DATA = {\n",
    "    '2015': datafolder / Path(\"clean-quotes-2015.bz2\"),\n",
    "    '2016': datafolder / Path(\"clean-quotes-2016.bz2\"),\n",
    "    '2017': datafolder / Path(\"clean-quotes-2017.bz2\"),\n",
    "    '2018': datafolder / Path(\"clean-quotes-2018.bz2\"),\n",
    "    '2019': datafolder / Path(\"clean-quotes-2019.bz2\"),\n",
    "    '2020': datafolder / Path(\"clean-quotes-2020.bz2\"),\n",
    "}\n",
    "\n",
    "ALL_YEARS = [\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"]\n",
    "\n",
    "def load_data(year, sample=True, sample_size=100_000):\n",
    "    if DATA[year].exists():\n",
    "        df = pd.read_csv(DATA[year], compression='bz2')\n",
    "        if sample:\n",
    "            df = df.sample(n=sample_size, random_state=1)\n",
    "        return df\n",
    "    else:\n",
    "        return None \n",
    "    \n",
    "wikidata_speakers = pd.read_parquet('data/speaker_attributes.parquet')\n",
    "wikidata_speakers.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bb10eec-ad21-4c4f-83bc-4a65aa7028a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2015...\n",
      "Done loading 2015\n",
      "Loading 2016...\n",
      "Done loading 2016\n",
      "Loading 2017...\n",
      "Done loading 2017\n",
      "Loading 2018...\n",
      "Done loading 2018\n",
      "Loading 2019...\n",
      "Done loading 2019\n",
      "Loading 2020...\n",
      "Done loading 2020\n"
     ]
    }
   ],
   "source": [
    "wanted_qids =  []\n",
    "for year in ALL_YEARS:\n",
    "    print(f\"Loading {year}...\")\n",
    "    df = load_data(year, sample=False)\n",
    "    print(f\"Done loading {year}\")\n",
    "    if df is not None:\n",
    "        qids = df.qids.tolist()\n",
    "        wanted_qids += [eval(qid)[0] for qid in qids if len(eval(qid)) == 1 and eval(qid)[0] in wikidata_speakers.index]\n",
    "    else:\n",
    "        Print(f\"could not find file for year {year}\")\n",
    "        \n",
    "speakers = wikidata_speakers.loc[wanted_qids]\n",
    "speakers = speakers[~speakers.index.duplicated(keep='first')]\n",
    "\n",
    "n_quotes_per_person = Counter(wanted_qids)\n",
    "speakers['n_quotes'] = speakers.index.map(n_quotes_per_person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd37a21f-8eb7-4220-9668-4bbf37c9129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers.to_csv(\"data/speakers.bz2\",compression='bz2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
