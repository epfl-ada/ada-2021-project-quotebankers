{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792d37f2-5ced-487a-a62b-d1fc018e6bac",
   "metadata": {},
   "source": [
    "# Who has a voice in the media?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f5913-1bd6-47fb-b803-5889a0d383c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pre-processing the dataset\n",
    "To start with, we remove the rows of the dataset where either the author or the quotation is NaN. In addition, we remove the authors who probability is lower than 50%. As our whole analysis of \"who has a voice in the media\" is all about the speaker and what it has said, it makes no sense to take these rows into account.\n",
    "\n",
    "Later, we also do a sanity controll and remove possible duplicate of rows with the same quote-ID as we obiously don't want to use exactly the same quote more than once in our analyzes. \n",
    "\n",
    "Finally, to reduce the dataset further we remove columns that we will not use for our analysis: _quoteID_, _speaker_, _probas_, _urls_, _phase_ and _numOccurrences_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d0b3f7-8038-40c9-974f-4bfef3a85fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(chunk, thresh=0.5):\n",
    "    \n",
    "    # Drop duplicate quoteIDs\n",
    "    nr_rows = chunk.shape[0]\n",
    "    chunk = chunk.drop_duplicates(subset=['quoteID'])\n",
    "    print('- Dropped {} duplicate rows with same quoteID;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Drop quotes where either speaker or quotation is None\n",
    "    nr_rows = chunk.shape[0]\n",
    "    chunk.replace(to_replace=['None'], value=np.nan, inplace=True)\n",
    "    chunk = chunk.dropna(axis=0, subset=['speaker', 'quotation'])\n",
    "    print('- Dropped {} rows with NaN speaker or quotation;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Drop rows where speakers has probability less than 50%\n",
    "    nr_rows = chunk.shape[0]\n",
    "    prob_filter = pd.Series([(float(chunk.iloc[i].probas[0][1]) > thresh) for i in range(nr_rows)])\n",
    "    prob_filter = pd.Series(prob_filter)\n",
    "    chunk = chunk[prob_filter.values]\n",
    "    print('- Dropped {} rows with speaker prob smaller than 50%;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Remove columns we don't care about\n",
    "    chunk = chunk.drop(columns=['quoteID', 'speaker', 'probas', 'urls', 'phase', 'numOccurrences'])\n",
    "\n",
    "    return chunk\n",
    "\n",
    "start_of_all = timer()\n",
    "read_from_file = 'data/quotes-2016.json.bz2'\n",
    "write_to_file = 'data/clean-quotes-2016.bz2'\n",
    "with pd.read_json(read_from_file, lines=True, compression='bz2', chunksize=1_000_000) as df_reader:\n",
    "    print('Started to process chunks...')\n",
    "    i = 0\n",
    "    for chunk in df_reader:\n",
    "        print('\\nProcessing new chunk...')\n",
    "        start = timer()\n",
    "        processed_chunk = clean_data(chunk)\n",
    "        processed_chunk.to_csv(write_to_file, compression='bz2', mode='a', index=False)\n",
    "        end = timer()\n",
    "        print('Done processing and saving chunk after {:.3f} seconds.'.format(end - start))\n",
    "        \n",
    "end_of_all = timer()\n",
    "print('\\nDONE processing all chunks and saving as csv after {:.3f} minutes.'.format((end_of_all - start_of_all) / 60))\n",
    "print('THE END!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43458793-75fc-44c6-9acb-263886e3a5d4",
   "metadata": {},
   "source": [
    "#### Short discussion around pre-processing\n",
    "Around one third of the original dataset has either a NaN quotation field, a NaN speaker, or a speaker with lower than 50% probability of having said that quote. Another one third of the original data is removed by the removal of the unwanted columns. Thus we are left with one third of the original dataset and still with full possibility of doing the wanted analysis\n",
    "\n",
    "Elsemore, it seems like there are no duplicates of quote-IDs in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
