{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "792d37f2-5ced-487a-a62b-d1fc018e6bac",
   "metadata": {},
   "source": [
    "# Who has a voice in the media?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63f5913-1bd6-47fb-b803-5889a0d383c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Pre-processing the Quotebank dataset\n",
    "To start with, we remove the rows of the dataset where either the author or the quotation is NaN. In addition, we remove the authors who probability is lower than 50%. As our whole analysis of \"who has a voice in the media\" is all about the speaker and what it has said, it makes no sense to take these rows into account.\n",
    "\n",
    "Later, we also do a sanity controll and remove possible duplicate of rows with the same quote-ID as we obiously don't want to use exactly the same quote more than once in our analyzes. \n",
    "\n",
    "Finally, to reduce the dataset further we remove columns that we will not use for our analysis: _quoteID_, _speaker_, _probas_, _urls_, _phase_ and _numOccurrences_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99d0b3f7-8038-40c9-974f-4bfef3a85fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started to process chunks...\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348775 rows with NaN speaker or quotation;\n",
      "- Dropped 32474 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.005 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348985 rows with NaN speaker or quotation;\n",
      "- Dropped 32299 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.004 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348778 rows with NaN speaker or quotation;\n",
      "- Dropped 32428 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.002 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348514 rows with NaN speaker or quotation;\n",
      "- Dropped 32595 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.004 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 347557 rows with NaN speaker or quotation;\n",
      "- Dropped 32485 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.000 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349038 rows with NaN speaker or quotation;\n",
      "- Dropped 32242 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.004 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349151 rows with NaN speaker or quotation;\n",
      "- Dropped 32310 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.002 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348451 rows with NaN speaker or quotation;\n",
      "- Dropped 32386 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.013 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348468 rows with NaN speaker or quotation;\n",
      "- Dropped 32421 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.001 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 347946 rows with NaN speaker or quotation;\n",
      "- Dropped 32403 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.012 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349276 rows with NaN speaker or quotation;\n",
      "- Dropped 32570 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.005 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348480 rows with NaN speaker or quotation;\n",
      "- Dropped 32543 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.000 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349176 rows with NaN speaker or quotation;\n",
      "- Dropped 32489 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.002 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348656 rows with NaN speaker or quotation;\n",
      "- Dropped 32127 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.000 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 347937 rows with NaN speaker or quotation;\n",
      "- Dropped 32208 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.003 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348633 rows with NaN speaker or quotation;\n",
      "- Dropped 32322 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.010 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349130 rows with NaN speaker or quotation;\n",
      "- Dropped 32487 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.011 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349290 rows with NaN speaker or quotation;\n",
      "- Dropped 32032 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.015 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348973 rows with NaN speaker or quotation;\n",
      "- Dropped 32274 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.005 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348629 rows with NaN speaker or quotation;\n",
      "- Dropped 32247 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 0.001 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 349223 rows with NaN speaker or quotation;\n",
      "- Dropped 32620 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.007 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348675 rows with NaN speaker or quotation;\n",
      "- Dropped 32329 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.016 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348090 rows with NaN speaker or quotation;\n",
      "- Dropped 32133 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.016 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348868 rows with NaN speaker or quotation;\n",
      "- Dropped 31914 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.010 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348348 rows with NaN speaker or quotation;\n",
      "- Dropped 32276 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.004 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 348372 rows with NaN speaker or quotation;\n",
      "- Dropped 32343 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.003 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 212871 rows with NaN speaker or quotation;\n",
      "- Dropped 19734 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after -0.003 seconds.\n",
      "\n",
      "DONE processing all chunks and saving as csv after -0.000 minutes.\n",
      "THE END!\n"
     ]
    }
   ],
   "source": [
    "from timeit import timeit as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def clean_data(chunk, thresh=0.5):\n",
    "    \n",
    "    # Drop duplicate quoteIDs\n",
    "    nr_rows = chunk.shape[0]\n",
    "    chunk = chunk.drop_duplicates(subset=['quoteID'])\n",
    "    print('- Dropped {} duplicate rows with same quoteID;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Drop quotes where either speaker or quotation is None\n",
    "    nr_rows = chunk.shape[0]\n",
    "    chunk.replace(to_replace=['None'], value=np.nan, inplace=True)\n",
    "    chunk = chunk.dropna(axis=0, subset=['speaker', 'quotation'])\n",
    "    print('- Dropped {} rows with NaN speaker or quotation;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Drop rows where speakers has probability less than 50%\n",
    "    nr_rows = chunk.shape[0]\n",
    "    prob_filter = pd.Series([(float(chunk.iloc[i].probas[0][1]) > thresh) for i in range(nr_rows)])\n",
    "    prob_filter = pd.Series(prob_filter)\n",
    "    chunk = chunk[prob_filter.values]\n",
    "    print('- Dropped {} rows with speaker prob smaller than 50%;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Remove columns we don't care about\n",
    "    chunk = chunk.drop(columns=['speaker', 'probas'])\n",
    "\n",
    "    return chunk\n",
    "\n",
    "start_of_all = timer()\n",
    "read_from_file = 'data/quotes-2015.json-002.bz2'\n",
    "write_to_file = 'data/clean-quotes-2015-updated.bz2'\n",
    "with pd.read_json(read_from_file, lines=True, compression='bz2', chunksize=1_000_000) as df_reader:\n",
    "    print('Started to process chunks...')\n",
    "    i = 0\n",
    "    for chunk in df_reader:\n",
    "        print('\\nProcessing new chunk...')\n",
    "        start = timer()\n",
    "        processed_chunk = clean_data(chunk)\n",
    "        processed_chunk.to_csv(write_to_file, compression='bz2', mode='a', index=False)\n",
    "        end = timer()\n",
    "        print('Done processing and saving chunk after {:.3f} seconds.'.format(end - start))\n",
    "        \n",
    "end_of_all = timer()\n",
    "print('\\nDONE processing all chunks and saving as csv after {:.3f} minutes.'.format((end_of_all - start_of_all) / 60))\n",
    "print('THE END!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43458793-75fc-44c6-9acb-263886e3a5d4",
   "metadata": {},
   "source": [
    "#### Short discussion around pre-processing\n",
    "Around one third of the original dataset has either a NaN quotation field, a NaN speaker, or a speaker with lower than 50% probability of having said that quote. Another one third of the original data is removed by the removal of the unwanted columns. Thus we are left with one third of the original dataset and still with full possibility of doing the wanted analysis\n",
    "\n",
    "Elsemore, it seems like there are no duplicates of quote-IDs in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186db5b0-6b8f-4ebe-9778-30efd28eba32",
   "metadata": {},
   "source": [
    "## 2. Creating the wikidata-speakers dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c06ca-02e8-461d-80e0-ee7d603c62ba",
   "metadata": {},
   "source": [
    "In order to analyse who has a voice in the media we add a new column \"n_quotes\" to the wikidata-dataset which is how many times that person is present in the quotebank dataset from 2015 to 2020. This new dataset is saved in \"speakers.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2399124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mathe\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "_ = nltk.download('wordnet')\n",
    "\n",
    "datafolder = Path(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd54c024-8614-4689-8a21-b843008d1e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = {\n",
    "    '2015': datafolder / Path(\"clean-quotes-2015.bz2\"),\n",
    "    '2016': datafolder / Path(\"clean-quotes-2016.bz2\"),\n",
    "    '2017': datafolder / Path(\"clean-quotes-2017.bz2\"),\n",
    "    '2018': datafolder / Path(\"clean-quotes-2018.bz2\"),\n",
    "    '2019': datafolder / Path(\"clean-quotes-2019.bz2\"),\n",
    "    '2020': datafolder / Path(\"clean-quotes-2020.bz2\"),\n",
    "}\n",
    "\n",
    "ALL_YEARS = [\"2015\",\"2016\",\"2017\",\"2018\",\"2019\",\"2020\"]\n",
    "\n",
    "def load_data(year, sample=True, sample_size=100_000):\n",
    "    if DATA[year].exists():\n",
    "        df = pd.read_csv(DATA[year], compression='bz2')\n",
    "        if sample:\n",
    "            df = df.sample(n=sample_size, random_state=1)\n",
    "        return df\n",
    "    else:\n",
    "        return None \n",
    "    \n",
    "wikidata_speakers = pd.read_parquet('data/speaker_attributes.parquet')\n",
    "wikidata_speakers.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bb10eec-ad21-4c4f-83bc-4a65aa7028a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 2015...\n",
      "Done loading 2015\n",
      "Loading 2016...\n",
      "Done loading 2016\n",
      "Loading 2017...\n",
      "Done loading 2017\n",
      "Loading 2018...\n",
      "Done loading 2018\n",
      "Loading 2019...\n",
      "Done loading 2019\n",
      "Loading 2020...\n",
      "Done loading 2020\n"
     ]
    }
   ],
   "source": [
    "wanted_qids =  []\n",
    "for year in ALL_YEARS:\n",
    "    print(f\"Loading {year}...\")\n",
    "    df = load_data(year, sample=False)\n",
    "    print(f\"Done loading {year}\")\n",
    "    if df is not None:\n",
    "        qids = df.qids.tolist()\n",
    "        wanted_qids += [eval(qid)[0] for qid in qids if len(eval(qid)) == 1 and eval(qid)[0] in wikidata_speakers.index]\n",
    "    else:\n",
    "        Print(f\"could not find file for year {year}\")\n",
    "        \n",
    "speakers = wikidata_speakers.loc[wanted_qids]\n",
    "speakers = speakers[~speakers.index.duplicated(keep='first')]\n",
    "\n",
    "n_quotes_per_person = Counter(wanted_qids)\n",
    "speakers['n_quotes'] = speakers.index.map(n_quotes_per_person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd37a21f-8eb7-4220-9668-4bbf37c9129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers.to_csv(\"data/speakers.bz2\",compression='bz2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819ee509",
   "metadata": {},
   "source": [
    "Now, let us clean this speaker dataset in the following ways:\n",
    "- change date_of_birth by age\n",
    "- drop columns that won't be used in our analysis\n",
    "- only keep first item of the list in the columns with list-values\n",
    "- drop the rows with any None values\n",
    "- remove the authors whose age is not between 0 and 200, and whose n_quotes is smaller than 1\n",
    "- construct new features using Wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a71c931c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "      <th>n_unique_quotes</th>\n",
       "      <th>n_quotes</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q270316</th>\n",
       "      <td>[Q30]</td>\n",
       "      <td>[Q6581072]</td>\n",
       "      <td>[Q82955]</td>\n",
       "      <td>4094</td>\n",
       "      <td>21060</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1253</th>\n",
       "      <td>[Q884]</td>\n",
       "      <td>[Q6581097]</td>\n",
       "      <td>[Q82955, Q193391]</td>\n",
       "      <td>12746</td>\n",
       "      <td>94704</td>\n",
       "      <td>77.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        nationality      gender         occupation  n_unique_quotes  n_quotes  \\\n",
       "Q270316       [Q30]  [Q6581072]           [Q82955]             4094     21060   \n",
       "Q1253        [Q884]  [Q6581097]  [Q82955, Q193391]            12746     94704   \n",
       "\n",
       "          age  \n",
       "Q270316  74.0  \n",
       "Q1253    77.0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers_features.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e87aedaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_quotes</th>\n",
       "      <th>n_unique_quotes</th>\n",
       "      <th>age</th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q270316</th>\n",
       "      <td>21060</td>\n",
       "      <td>4094</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Q30</td>\n",
       "      <td>Q6581072</td>\n",
       "      <td>Q82955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1253</th>\n",
       "      <td>94704</td>\n",
       "      <td>12746</td>\n",
       "      <td>77.0</td>\n",
       "      <td>Q884</td>\n",
       "      <td>Q6581097</td>\n",
       "      <td>Q82955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q19874690</th>\n",
       "      <td>1207</td>\n",
       "      <td>205</td>\n",
       "      <td>62.0</td>\n",
       "      <td>Q408</td>\n",
       "      <td>Q6581097</td>\n",
       "      <td>Q39631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q5271548</th>\n",
       "      <td>1587</td>\n",
       "      <td>573</td>\n",
       "      <td>83.0</td>\n",
       "      <td>Q30</td>\n",
       "      <td>Q6581072</td>\n",
       "      <td>Q1930187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q2287947</th>\n",
       "      <td>132971</td>\n",
       "      <td>16482</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Q30</td>\n",
       "      <td>Q6581097</td>\n",
       "      <td>Q11303721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           n_quotes  n_unique_quotes   age nationality    gender occupation\n",
       "Q270316       21060             4094  74.0         Q30  Q6581072     Q82955\n",
       "Q1253         94704            12746  77.0        Q884  Q6581097     Q82955\n",
       "Q19874690      1207              205  62.0        Q408  Q6581097     Q39631\n",
       "Q5271548       1587              573  83.0         Q30  Q6581072   Q1930187\n",
       "Q2287947     132971            16482  28.0         Q30  Q6581097  Q11303721"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataframes\n",
    "speakers = pd.read_json('data/speakers.json.bz2', compression='bz2')\n",
    "\n",
    "# Change date_of_birth to age\n",
    "ages = []\n",
    "for date in speakers.date_of_birth.values:\n",
    "    if not date is None:\n",
    "        ages.append(datetime.now().year - int(date[0][1:5]))\n",
    "    else:\n",
    "        ages.append(None)\n",
    "\n",
    "speakers['age'] = ages\n",
    "\n",
    "# Drop uninteresting columns\n",
    "speakers_features = speakers.drop(columns=['aliases', 'label', 'US_congress_bio_ID', \n",
    "                                           'lastrevid', 'type', \n",
    "                                           'candidacy', 'academic_degree',\n",
    "                                           'date_of_birth', 'religion',\n",
    "                                           'ethnic_group', 'party'\n",
    "                                           ])\n",
    "\n",
    "# Keep only first instance of occupation, nationality, gender\n",
    "speakers_features_full = pd.DataFrame()\n",
    "speakers_features_full['n_quotes'] = speakers_features['n_quotes']\n",
    "speakers_features_full['n_unique_quotes'] = speakers_features['n_unique_quotes']\n",
    "speakers_features_full['age'] = speakers_features['age']\n",
    "\n",
    "for name, values in speakers_features.iteritems():\n",
    "    if name not in ['n_quotes', 'age', 'n_unique_quotes']:\n",
    "        updated_values = []\n",
    "        for val in values:\n",
    "            if not val is None:\n",
    "                updated_values.append(val[0])\n",
    "            else:\n",
    "                updated_values.append(None)\n",
    "        speakers_features_full[name] = updated_values\n",
    "\n",
    "speakers_features_preprocessed_ = speakers_features_full.dropna(axis=0) # remove row if any column value is None\n",
    "\n",
    "# Remove the authors whose age is not between 0 and 150, and whose n_quotes is smaller than 1\n",
    "speakers_features_preprocessed = speakers_features_preprocessed_[(speakers_features_preprocessed_.age > 0) \n",
    "                                                               & (speakers_features_preprocessed_.age < 150) \n",
    "                                                               & (speakers_features_preprocessed_.n_quotes > 0)]\n",
    "speakers_features_preprocessed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3dd6d9",
   "metadata": {},
   "source": [
    "Construct new occupation-related features using **Wordnet**. Every occupation has its semantic difference to the top 8 pre-defined occupations calculated with Wordnet. This way, there is no need to one-hot encode the roughly 2400 occupations when doing the clustering, but we can rather keep it down to 8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0c622c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-e8fc9fb8b739>:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  speakers_features_preprocessed[f'{top_occupation}_score'] = None\n",
      "C:\\Users\\mathe\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexing.py:1797: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, v, pi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(330297, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_quotes</th>\n",
       "      <th>n_unique_quotes</th>\n",
       "      <th>age</th>\n",
       "      <th>nationality</th>\n",
       "      <th>gender</th>\n",
       "      <th>politician_score</th>\n",
       "      <th>athlete_score</th>\n",
       "      <th>actor_score</th>\n",
       "      <th>lawyer_score</th>\n",
       "      <th>researcher_score</th>\n",
       "      <th>journalist_score</th>\n",
       "      <th>musician_score</th>\n",
       "      <th>businessperson_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Q270316</th>\n",
       "      <td>21060</td>\n",
       "      <td>4094</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Q30</td>\n",
       "      <td>Q6581072</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q1253</th>\n",
       "      <td>94704</td>\n",
       "      <td>12746</td>\n",
       "      <td>77.0</td>\n",
       "      <td>Q884</td>\n",
       "      <td>Q6581097</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q19874690</th>\n",
       "      <td>1207</td>\n",
       "      <td>205</td>\n",
       "      <td>62.0</td>\n",
       "      <td>Q408</td>\n",
       "      <td>Q6581097</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.114286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q5271548</th>\n",
       "      <td>1587</td>\n",
       "      <td>573</td>\n",
       "      <td>83.0</td>\n",
       "      <td>Q30</td>\n",
       "      <td>Q6581072</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q2287947</th>\n",
       "      <td>132971</td>\n",
       "      <td>16482</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Q30</td>\n",
       "      <td>Q6581097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           n_quotes  n_unique_quotes   age nationality    gender  \\\n",
       "Q270316       21060             4094  74.0         Q30  Q6581072   \n",
       "Q1253         94704            12746  77.0        Q884  Q6581097   \n",
       "Q19874690      1207              205  62.0        Q408  Q6581097   \n",
       "Q5271548       1587              573  83.0         Q30  Q6581072   \n",
       "Q2287947     132971            16482  28.0         Q30  Q6581097   \n",
       "\n",
       "          politician_score athlete_score actor_score lawyer_score  \\\n",
       "Q270316                0.2           0.0         0.0          0.0   \n",
       "Q1253                  0.2           0.0         0.0          0.0   \n",
       "Q19874690         0.114286      0.114286    0.109091     0.109091   \n",
       "Q5271548               0.0           0.0         0.0          0.0   \n",
       "Q2287947               0.0           0.2         0.0          0.0   \n",
       "\n",
       "          researcher_score journalist_score musician_score  \\\n",
       "Q270316                0.0              0.0            0.0   \n",
       "Q1253                  0.0              0.0            0.0   \n",
       "Q19874690         0.114286         0.109091       0.109091   \n",
       "Q5271548               0.0              0.2            0.0   \n",
       "Q2287947               0.0              0.0            0.0   \n",
       "\n",
       "          businessperson_score  \n",
       "Q270316                    0.0  \n",
       "Q1253                      0.0  \n",
       "Q19874690             0.114286  \n",
       "Q5271548                   0.0  \n",
       "Q2287947                   0.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qwikidata.linked_data_interface import get_entity_dict_from_api\n",
    "\n",
    "def map_qid_occupations():\n",
    "    \"\"\"\n",
    "    Maps the qids to occupation names. Change some occupation names so that they match the\n",
    "    existing nouns in Wordnet.\n",
    "    \"\"\"\n",
    "    qids = speakers_features_preprocessed.occupation.unique().tolist()\n",
    "    qid_occupation_map = {}\n",
    "    for qid in qids:\n",
    "        if qid not in qid_occupation_map:\n",
    "            entity = get_entity_dict_from_api(qid)['labels']\n",
    "            if 'en' not in entity: continue\n",
    "            occupation_name = entity['en']['value']\n",
    "            if occupation_name == 'association football player': occupation_name = 'soccer_player'\n",
    "            elif occupation_name == 'American football player': occupation_name = 'football_player'\n",
    "            elif occupation_name == 'rugby union player': occupation_name = 'football_player'\n",
    "            elif occupation_name == 'rugby league player': occupation_name = 'football_player'\n",
    "            elif occupation_name == 'ice hockey player': occupation_name = 'athlete'\n",
    "            elif occupation_name == 'boxer': occupation_name = 'athlete'\n",
    "            elif occupation_name == 'golfer': occupation_name = 'athlete'\n",
    "            elif occupation_name == 'business magnate': occupation_name = 'businessperson'\n",
    "            elif occupation_name == 'business executive': occupation_name = 'businessperson'\n",
    "            elif occupation_name == 'singer-songwriter': occupation_name = 'musician'\n",
    "            elif occupation_name == 'composer': occupation_name = 'musician'\n",
    "            elif occupation_name == 'film-director': occupation_name = 'film_director'\n",
    "            elif occupation_name == 'film producer': occupation_name = 'film_director'\n",
    "            elif occupation_name == 'film actor': occupation_name = 'actor'\n",
    "            elif occupation_name == 'television actor': occupation_name = 'actor'\n",
    "            elif occupation_name == 'comedian': occupation_name = 'actor'\n",
    "            elif occupation_name == 'diplomat': occupation_name = 'politician'\n",
    "            elif occupation_name == 'philosopher': occupation_name = 'researcher'\n",
    "            elif occupation_name == 'economist': occupation_name = 'researcher'\n",
    "            qid_occupation_map[qid] = occupation_name\n",
    "    return qid_occupation_map\n",
    "    \n",
    "# UNCOMMENT ROW BELOW TO GET qid <-> occupation_name map\n",
    "qid_occupation_map = map_qid_occupations()\n",
    "\n",
    "top_occupations = ['politician', 'athlete', 'actor', 'lawyer', 'researcher', 'journalist', 'musician', 'businessperson']\n",
    "import numpy as np\n",
    "\n",
    "def get_wordnet_similarity_to(top_occupations, qid_occupation_map, thresh=0.7):\n",
    "    \"\"\"\n",
    "    Calculates the similarities of all unique occupation qids to the pre-defined top occupations.\n",
    "    Returns a dictionary where the keys are each unique occupation qid, and the value is the similarity\n",
    "    to each top occupation. E.g.: qid_simlarities_map = {'Q1234': [similarity_to_politician, similarity_to_athlete, ..., similarity_to_businessperson], ...}.\n",
    "    \"\"\"\n",
    "    unique_qids = speakers_features_preprocessed.occupation.unique().tolist()\n",
    "    qid_similarities_map = {}\n",
    "    for qid in unique_qids:\n",
    "        if qid in qid_occupation_map and wn.synsets(qid_occupation_map[qid]): \n",
    "            dist_to_top_occupations = np.zeros((8, )) #{occupation: None for occupation in top_occupations}\n",
    "            qid_synset_obj = wn.synsets(qid_occupation_map[qid])[0]\n",
    "            for i, occ in enumerate(top_occupations):\n",
    "                top_occ_synset_obj = wn.synsets(occ)[0]\n",
    "                similarity_to_top_occupation = top_occ_synset_obj.wup_similarity(qid_synset_obj)\n",
    "                dist_to_top_occupations[i] = similarity_to_top_occupation\n",
    "            if np.max(dist_to_top_occupations) >= thresh:\n",
    "                dist_to_top_occupations[dist_to_top_occupations != np.max(dist_to_top_occupations)] = 0\n",
    "                dist_to_top_occupations[dist_to_top_occupations == np.max(dist_to_top_occupations)] = 1\n",
    "            qid_similarities_map[qid] = dist_to_top_occupations / 5 # divide by 5 to compensate the fact that the semantic distance to the top occupations are lower than threshold, this means it is uncertain in which it best fits\n",
    "    return qid_similarities_map\n",
    "            \n",
    "qid_similarities_map = get_wordnet_similarity_to(top_occupations, qid_occupation_map)\n",
    "\n",
    "# Create new columns: politician_score, athlete_score, actor_score, lawyer_score, researcher_score, journalist_score, musician_score, businessperson_score\n",
    "for top_occupation in top_occupations:\n",
    "    speakers_features_preprocessed[f'{top_occupation}_score'] = None\n",
    "speakers_features_preprocessed.head(20)\n",
    "\n",
    "unique_occupation_qids = speakers_features_preprocessed.occupation.unique().tolist()\n",
    "for occ_qid in qid_similarities_map:\n",
    "    speakers_features_preprocessed.loc[speakers_features_preprocessed['occupation'] == occ_qid, \n",
    "                                      ['politician_score', 'athlete_score', 'actor_score', 'lawyer_score', \n",
    "                                       'researcher_score', 'journalist_score', 'musician_score', 'businessperson_score']] = qid_similarities_map[occ_qid]\n",
    "\n",
    "speakers_features_preprocessed_final = speakers_features_preprocessed.drop(columns=['occupation']).dropna()\n",
    "print(speakers_features_preprocessed_final.shape)\n",
    "speakers_features_preprocessed_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec52a0",
   "metadata": {},
   "source": [
    "Save to JSON. This will be used all through our analysis: in clustering.ipynb, what.ipynb, and how.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_features_preprocessed_final.to_json('data/speakers_8_occupations', compression='bz2')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0050bdc898b9a679c387244c11e8057e7ae4f22360b32a2e55d1f190f6800284"
  },
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
