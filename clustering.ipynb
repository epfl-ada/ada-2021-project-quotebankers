{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96aed65-125a-4489-8c08-622e99d0ca60",
   "metadata": {},
   "source": [
    "# _Who has a voice in the media?_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1593c269-a4a4-4638-8b8c-08f27493e276",
   "metadata": {},
   "source": [
    "## 1. Pre-processing the dataset\n",
    "In this study of \"_Who has a voice in the media_\", the **speaker identity and what it said is vital**. Thus, we remove the following rows from the original dataset:\n",
    "- rows where either the author or the quotation is NaN; \n",
    "- rows where the author has probability lower than 50%. \n",
    "\n",
    "Later, we also do a sanity controll and **remove possible duplicate of rows** with the same quote-ID as we obiously don't want to use exactly the same quote more than once in our analyzes. \n",
    "\n",
    "Finally, to reduce the dataset further we **remove columns** that we will not use for our analysis: _quoteID_, _speaker_, _probas_, _urls_, _phase_ and _numOccurrences_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e15f9ce-bf3e-4ac7-bd12-561bdd5fbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb7cc0-60c0-45c3-8c11-10233b200116",
   "metadata": {},
   "source": [
    "## 2. Initial analysis\n",
    "Here, we do initial studies on the content of the dataset. For instance we plot the following information about the speakers:\n",
    "- occupation;\n",
    "- gender;\n",
    "- age;\n",
    "- ethnicity;\n",
    "- top 20 speakers.\n",
    "**OBS! For practical reasons, in the initial analysis in Milestone 2, we randomly picked 100,000 quotations of each year instead of dealing with the whole data. The code and the analysis will basically remain the same but only need to be run for a longer time.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083d5e2-bf21-4108-83ec-1b0d2aaa35f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = Path(\"data\")\n",
    "\n",
    "DATA = {\n",
    "    '2015': 'data/clean-quotes-2015.bz2',\n",
    "    '2016': 'data/clean-quotes-2016.bz2',\n",
    "    '2017': 'data/clean-quotes-2017.bz2',\n",
    "    '2018': 'data/clean-quotes-2018.bz2',\n",
    "    '2019': 'data/clean-quotes-2019.bz2',\n",
    "    '2020': 'data/clean-quotes-2020.bz2',\n",
    "}\n",
    "\n",
    "ALL_YEARS = ['2015', '2016', '2017', '2018', '2019', '2020']\n",
    "\n",
    "def load_data(year, sample=True, sample_size=100_000):\n",
    "    year_file = Path(DATA[year])\n",
    "    if year_file.exists():\n",
    "        df = pd.read_csv(DATA[year], compression='bz2')\n",
    "        if sample:\n",
    "            df = df.sample(n=sample_size, random_state=1)\n",
    "    else:\n",
    "        return None \n",
    "    \n",
    "wikidata_speakers = pd.read_parquet('data/speaker_attributes.parquet')\n",
    "wikidata_speakers.set_index('id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d390fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA['2020'], compression='bz2')\n",
    "df = df.sample(n=100_000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e29f4e-b580-4f7b-93b1-269321ece587",
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = df.qids.tolist()\n",
    "wanted_qids = [eval(qid)[0] for qid in qids if len(eval(qid)) == 1 and eval(qid)[0] in wikidata_speakers.index]\n",
    "speakers = wikidata_speakers.loc[wanted_qids]\n",
    "speakers = speakers[~speakers.index.duplicated(keep='first')]\n",
    "\n",
    "n_quotes_per_person = Counter(wanted_qids)\n",
    "speakers['n_quotes'] = speakers.index.map(n_quotes_per_person)\n",
    "\n",
    "ages = []\n",
    "for date in speakers.date_of_birth.values:\n",
    "    if not date is None:\n",
    "        ages.append(datetime.now().year - int(date[0][1:5]))\n",
    "    else:\n",
    "        ages.append(None)\n",
    "\n",
    "speakers['age'] = ages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9007ddd",
   "metadata": {},
   "source": [
    "### 3. Pre-process dataset\n",
    "- Only keep meaningful features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aee935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The percentage of speakers with US_congress_bio_ID is {100 * len(speakers.US_congress_bio_ID.value_counts().index) / len(speakers.index) :.3f}%')\n",
    "print(f'The percentage of speakers with candidacy is {100 * sum(speakers.candidacy.value_counts().values) / len(speakers.index):.3f}%')\n",
    "print(f'The percentage of speakers with ethnic_group is {100 * sum(speakers.ethnic_group.value_counts().values) / len(speakers.index):.3f}%')\n",
    "print(f'The percentage of speakers with religion is {100 * sum(speakers.religion.value_counts().values) / len(speakers.index):.3f}%')\n",
    "print(f'The percentage of speakers with academic_degree is {100 * sum(speakers.academic_degree.value_counts().values) / len(speakers.index):.3f}%')\n",
    "print(f'The percentage of speakers with party is {100 * sum(speakers.party.value_counts().values) / len(speakers.index):.3f}%')\n",
    "print(f'The percentage of speakers with nationality is {100 * sum(speakers.nationality.value_counts().values) / len(speakers.index):.3f}%')\n",
    "print(f'The percentage of speakers with gender is {100 * sum(speakers.gender.value_counts().values) / len(speakers.index):.3f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b525af0",
   "metadata": {},
   "source": [
    "Drop the columns \n",
    "- that are particular to the speaker and don't add any value to the clustering (alisaes, label, US_congress_ID) ;\n",
    "- that don't add value to clustering (lastrevid, type);\n",
    "- that contain too little data to draw any conclusions (candidacy - 2.9%, academic_degree - 1.3%);\n",
    "- used to produce the new ages column (date_of_birth). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fd782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_features = speakers.drop(columns=['aliases', 'label', 'US_congress_bio_ID', \n",
    "                                           'lastrevid', 'type', \n",
    "                                           'candidacy', 'academic_degree', \n",
    "                                           'date_of_birth'])\n",
    "speakers_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e3ed3f",
   "metadata": {},
   "source": [
    "More pre-processing: make the None names to 'Unknown', otherwise the KPrototype doesn't run. This way, the 'Unknown' class becomes a categorical variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb43e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_features_preprocessed = pd.DataFrame()\n",
    "speakers_features_preprocessed['n_quotes'] = speakers_features['n_quotes']\n",
    "speakers_features_preprocessed['age'] = speakers_features['age'].fillna(speakers_features['age'].median())\n",
    "\n",
    "for name, values in speakers_features.iteritems():\n",
    "    # Remove all None values from categorical rows, and keep only first instance of list occupation, nationality\n",
    "    if name not in ['n_quotes', 'age']:\n",
    "        updated_values = []\n",
    "        for val in values:\n",
    "            if not val is None:\n",
    "                updated_values.append(val[0])\n",
    "            else:\n",
    "                updated_values.append('Unknown')\n",
    "        speakers_features_preprocessed[name] = updated_values\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4130bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_features_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00acd9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(speakers_features_preprocessed.n_quotes.isna().sum() == 0)\n",
    "assert(speakers_features_preprocessed.age.isna().sum() == 0)\n",
    "assert(speakers_features_preprocessed.nationality.isna().sum() == 0)\n",
    "assert(speakers_features_preprocessed.gender.isna().sum() == 0)\n",
    "assert(speakers_features_preprocessed.ethnic_group.isna().sum() == 0)\n",
    "assert(speakers_features_preprocessed.occupation.isna().sum() == 0)\n",
    "assert(speakers_features_preprocessed.party.isna().sum() == 0)\n",
    "assert(speakers_features_preprocessed.religion.isna().sum() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7812ea03",
   "metadata": {},
   "source": [
    "### 4. Cluster with K-prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab4f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kprototypes import KPrototypes\n",
    "\n",
    "def plot_sse(features, start=5, end=7):\n",
    "    sse = []\n",
    "    for k in range(start, end):\n",
    "        # Assign the labels to the clusters\n",
    "        kproto = KPrototypes(n_clusters=k, random_state=10).fit(features, categorical=[2, 3, 4, 5, 6, 7])\n",
    "        sse.append({\"k\": k, \"sse\": kproto.cost_ })\n",
    "\n",
    "    sse = pd.DataFrame(sse)\n",
    "    # Plot the data\n",
    "    plt.plot(sse.k, sse.sse)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sum of Squared Errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_sse(speakers_features_preprocessed)\n",
    "kproto = KPrototypes(n_clusters=2, random_state=10, n_jobs=-1).fit(speakers_features_preprocessed, categorical=[2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d271f",
   "metadata": {},
   "source": [
    "### 5. Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72ffcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = speakers_features_preprocessed.drop(columns=['nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'religion'])\n",
    "categorical = speakers_features_preprocessed.drop(columns=['n_quotes', 'age'])\n",
    "categorical = pd.get_dummies(categorical)\n",
    "\n",
    "# # # Embedding numerical & categorical\n",
    "fit1 = umap.UMAP(metric='l2').fit(numerical)\n",
    "fit2 = umap.UMAP(metric='dice').fit(categorical)\n",
    "\n",
    "categorical_weight = len(categorical.columns) / (len(categorical.columns) + len(numerical.columns))\n",
    "\n",
    "# # Augmenting the numerical embedding with categorical\n",
    "intersection = umap.umap_.general_simplicial_set_intersection(fit1.graph_, fit2.graph_, weight=categorical_weight)\n",
    "# intersection = umap.umap_.reset_local_connectivity(intersection)\n",
    "\n",
    "# embedding = umap.umap_.simplicial_set_embedding(fit1._raw_data, intersection, fit1.n_components, \n",
    "#                                                 fit1._initial_alpha, fit1._a, fit1._b, \n",
    "#                                                 fit1.repulsion_strength, fit1.negative_sample_rate, \n",
    "#                                                 200, 'random', np.random, fit1.metric, \n",
    "#                                                 fit1._metric_kwds, False,\n",
    "#                                                 {}, False)\n",
    "\n",
    "# plt.figure(figsize=(20, 10))\n",
    "# plt.scatter(*embedding.T, s=2, cmap='Spectral', alpha=1.0)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91514f2c",
   "metadata": {},
   "source": [
    "### 6. Visualize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bc0199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(4,4), sharey=True)\n",
    "\n",
    "# # Reduce data to 2 dimensions with t-SNE\n",
    "# speakers_tsne = TSNE(n_components=2, random_state=0).fit_transform(speakers_features_preprocessed)\n",
    "\n",
    "# # Plot the data reduced in 2d space with t-SNE\n",
    "# axs.scatter(speakers_tsne[:,0],  speakers_tsne[:,1],  c=kproto.labels_, alpha=0.6)\n",
    "# axs.set_title('Predicted labels')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
