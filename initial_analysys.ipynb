{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96aed65-125a-4489-8c08-622e99d0ca60",
   "metadata": {},
   "source": [
    "# Who has a voice in the media?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb7cc0-60c0-45c3-8c11-10233b200116",
   "metadata": {},
   "source": [
    "## 1. Pre-processing the dataset\n",
    "To start with, we remove the rows of the dataset where either the author or the quotation is NaN. As our whole analysis of \"who has a voice in the media\" is all about the speaker and what it has said, it makes no sense to take these rows into account.\n",
    "\n",
    "Later, we also do a sanity controll and remove possible duplicate of rows with the same quote-ID as we obiously don't want to use exactly the same quote more than once in our analyzes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dad80c-8f67-4e37-b510-53d1fa626c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eb55ef4-d700-4c3a-86e6-27ea2af192c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started to process chunks...\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 343700 rows with NaN speaker or quotation;\n",
      "- Dropped 30524 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 89.029 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 343778 rows with NaN speaker or quotation;\n",
      "- Dropped 30897 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 125.076 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 343353 rows with NaN speaker or quotation;\n",
      "- Dropped 30405 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 95.449 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 343472 rows with NaN speaker or quotation;\n",
      "- Dropped 30511 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 104.026 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 342413 rows with NaN speaker or quotation;\n",
      "- Dropped 30809 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 125.618 seconds.\n",
      "\n",
      "Processing new chunk...\n",
      "- Dropped 0 duplicate rows with same quoteID;\n",
      "- Dropped 84128 rows with NaN speaker or quotation;\n",
      "- Dropped 7493 rows with speaker prob smaller than 50%;\n",
      "Done processing and saving chunk after 22.674 seconds.\n",
      "Done processing all chunks and saving as csv after 31.622 minutes.\n"
     ]
    }
   ],
   "source": [
    "def clean_data(chunk, thresh=0.5):\n",
    "    \n",
    "    # Drop duplicate quoteIDs\n",
    "    nr_rows = chunk.shape[0]\n",
    "    chunk = chunk.drop_duplicates(subset=['quoteID'])\n",
    "    print('- Dropped {} duplicate rows with same quoteID;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Drop quotes where either speaker or quotation is None\n",
    "    nr_rows = chunk.shape[0]\n",
    "    chunk.replace(to_replace=['None'], value=np.nan, inplace=True)\n",
    "    chunk = chunk.dropna(axis=0, subset=['speaker', 'quotation'])\n",
    "    print('- Dropped {} rows with NaN speaker or quotation;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Drop rows where speakers has probability less than 50%\n",
    "    nr_rows = chunk.shape[0]\n",
    "    prob_filter = pd.Series([(float(chunk.iloc[i].probas[0][1]) > thresh) for i in range(nr_rows)])\n",
    "#     prob_filter = []\n",
    "#     for i in range(nr_rows):\n",
    "#         prob = float(chunk.iloc[i].probas[0][1])\n",
    "#         if prob < thresh:\n",
    "#             prob_filter.append(False)\n",
    "#         else:\n",
    "#             prob_filter.append(True)\n",
    "    prob_filter = pd.Series(prob_filter)\n",
    "    chunk = chunk[prob_filter.values]\n",
    "    print('- Dropped {} rows with speaker prob smaller than 50%;'.format(nr_rows - chunk.shape[0]))\n",
    "    \n",
    "    # Remove columns we don't care about\n",
    "    chunk = chunk.drop(columns=['quoteID', 'speaker', 'probas', 'urls', 'phase', 'numOccurrences'])\n",
    "\n",
    "    return chunk\n",
    "\n",
    "start_of_all = timer()\n",
    "read_from_file = 'data/quotes-2020.json.bz2'\n",
    "write_to_file = 'data/clean-quotes-2020.bz2'\n",
    "with pd.read_json(read_from_file, lines=True, compression='bz2', chunksize=1_000_000) as df_reader:\n",
    "    print('Started to process chunks...')\n",
    "    i = 0\n",
    "    for chunk in df_reader:\n",
    "#         if i > 1:\n",
    "#             break\n",
    "#         i += 1\n",
    "        print('\\nProcessing new chunk...')\n",
    "        start = timer()\n",
    "        processed_chunk = clean_data(chunk)\n",
    "        processed_chunk.to_csv(write_to_file, compression='bz2', mode='a', index=False)\n",
    "        end = timer()\n",
    "        print('Done processing and saving chunk after {:.3f} seconds.'.format(end - start))\n",
    "        \n",
    "end_of_all = timer()\n",
    "print('\\nDONE processing all chunks and saving as csv after {:.3f} minutes.'.format((end_of_all - start_of_all) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0db18f-05a9-49e2-996d-cfbb76367b0b",
   "metadata": {},
   "source": [
    "#### Short discussion\n",
    "Around one third of the dataset seems to have either a NaN speaker or quotation field. We should rethink if there is a way to use this data despite of missing speaker or quotation fields.\n",
    "\n",
    "Elsemore, it seems like there are no duplicates of quote-IDs in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c942ebc-3002-4b31-aeab-a1a3c7a47ea9",
   "metadata": {},
   "source": [
    "## 2. Initial analyzes\n",
    "Here, we do initial studies on the dataset. For instance we plot the following information about the speakers:\n",
    "- gender;\n",
    "- age;\n",
    "- ethnicity;\n",
    "- profession.\n",
    "\n",
    "Also, we do analyzes on the content of the quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29efc634-d3e0-432a-995f-009f55320790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of males and females\n",
    "# profession\n",
    "# age\n",
    "# ethnicity\n",
    "# clean if person has several references in wikidata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14208cb6-06f0-4991-9d51-eeb82a3f399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/clean-quotes-2020.bz2', compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fcd6e-3f5e-4bd5-b9f0-0fb9a350a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "parq = pd.read_parquet('data/speaker_attributes.parquet-20211104T133449Z-001.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8123498-418c-4828-bd66-e6e372fd86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['probas', 'speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f90b47a-9890-4192-9306-752435193fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quotation</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ Department of Homeland Security ] was livid ...</td>\n",
       "      <td>['Q367796']</td>\n",
       "      <td>2020-01-16 12:00:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ I met them ] when they just turned 4 and 7. ...</td>\n",
       "      <td>['Q20684375']</td>\n",
       "      <td>2020-01-24 20:37:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ The delay ] will have an impact [ on Slough ...</td>\n",
       "      <td>['Q5268447']</td>\n",
       "      <td>2020-01-17 13:03:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ The scheme ] treats addiction as an illness ...</td>\n",
       "      <td>['Q4864119']</td>\n",
       "      <td>2020-04-02 14:18:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ These ] actions will allow households who ha...</td>\n",
       "      <td>['Q816459']</td>\n",
       "      <td>2020-03-19 19:14:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282966</th>\n",
       "      <td>you're going to take care of the gun problem w...</td>\n",
       "      <td>['Q6279']</td>\n",
       "      <td>2020-03-03 15:49:51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282967</th>\n",
       "      <td>you're seeing a young team that's maturing, th...</td>\n",
       "      <td>['Q18115465']</td>\n",
       "      <td>2020-02-24 05:00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282968</th>\n",
       "      <td>You're talking about African-Americans, right?...</td>\n",
       "      <td>['Q3635235']</td>\n",
       "      <td>2020-02-07 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282969</th>\n",
       "      <td>You've got to sometimes take that leap of fait...</td>\n",
       "      <td>['Q896796']</td>\n",
       "      <td>2020-02-04 14:47:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3282970</th>\n",
       "      <td>You've got to think that long term, it augurs ...</td>\n",
       "      <td>['Q18530304']</td>\n",
       "      <td>2020-02-24 04:30:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3282971 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 quotation           qids  \\\n",
       "0        [ Department of Homeland Security ] was livid ...    ['Q367796']   \n",
       "1        [ I met them ] when they just turned 4 and 7. ...  ['Q20684375']   \n",
       "2        [ The delay ] will have an impact [ on Slough ...   ['Q5268447']   \n",
       "3        [ The scheme ] treats addiction as an illness ...   ['Q4864119']   \n",
       "4        [ These ] actions will allow households who ha...    ['Q816459']   \n",
       "...                                                    ...            ...   \n",
       "3282966  you're going to take care of the gun problem w...      ['Q6279']   \n",
       "3282967  you're seeing a young team that's maturing, th...  ['Q18115465']   \n",
       "3282968  You're talking about African-Americans, right?...   ['Q3635235']   \n",
       "3282969  You've got to sometimes take that leap of fait...    ['Q896796']   \n",
       "3282970  You've got to think that long term, it augurs ...  ['Q18530304']   \n",
       "\n",
       "                        date  \n",
       "0        2020-01-16 12:00:13  \n",
       "1        2020-01-24 20:37:09  \n",
       "2        2020-01-17 13:03:00  \n",
       "3        2020-04-02 14:18:20  \n",
       "4        2020-03-19 19:14:00  \n",
       "...                      ...  \n",
       "3282966  2020-03-03 15:49:51  \n",
       "3282967  2020-02-24 05:00:28  \n",
       "3282968  2020-02-07 00:00:00  \n",
       "3282969  2020-02-04 14:47:00  \n",
       "3282970  2020-02-24 04:30:00  \n",
       "\n",
       "[3282971 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
